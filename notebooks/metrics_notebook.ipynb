{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GQaSQNQQvbo"
      },
      "source": [
        "# Metrics\n",
        "\n",
        "Evaluates a trained model accordingly to the metrics specified on the paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER7854z7Qvbw",
        "outputId": "e1519269-d84c-4298-e995-a61c1cbba019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kymatio in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kymatio) (21.0)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.7/dist-packages (from kymatio) (5.0.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.4.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kymatio) (2.4.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.7/dist-packages (0.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"../src\")\n",
        "\n",
        "from ModelHandler import ModelHandler\n",
        "import pickle\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "configs = {\n",
        "    \"N_GRIDS\": 5, \n",
        "    \"SIGNAL_BASE_LENGTH\": 12800, \n",
        "    \"N_CLASS\": 26, \n",
        "    \"USE_NO_LOAD\": False, \n",
        "    \"AUGMENTATION_RATIO\": 5, \n",
        "    \"MARGIN_RATIO\": 0.15, \n",
        "    \"DATASET_PATH\": \"../Synthetic_Full_iHall.hdf5\",\n",
        "    \"TRAIN_SIZE\": 0.8,\n",
        "    \"FOLDER_PATH\": \"drive/MyDrive/YOLO_NILM/final/001/\",\n",
        "    \"FOLDER_DATA_PATH\": \"drive/MyDrive/YOLO_NILM/final/001/\", \n",
        "    \"N_EPOCHS_TRAINING\": 250,\n",
        "    \"INITIAL_EPOCH\": 0,\n",
        "    \"TOTAL_MAX_EPOCHS\": 250,\n",
        "    \"SNRdb\": None # Noise level on db\n",
        "}\n",
        "\n",
        "folderPath = configs[\"FOLDER_PATH\"]\n",
        "folderDataPath = configs[\"FOLDER_DATA_PATH\"]\n",
        "signalBaseLength = configs[\"SIGNAL_BASE_LENGTH\"]\n",
        "ngrids = configs[\"N_GRIDS\"]\n",
        "trainSize = configs[\"TRAIN_SIZE\"]\n",
        "\n",
        "dict_data = pickle.load(open(folderDataPath + \"data.p\", \"rb\")) # Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vomuxuyDZ9Pe"
      },
      "source": [
        "## Choose best performing model\n",
        "\n",
        "At this point, the model with best performance under the validation set is chosen.\n",
        "\n",
        "In order to make this choice, the average between f1 macro is verified.\n",
        "\n",
        "$$\n",
        "F_1 = \\frac{F1_{ON} + F1_{OFF} + F1_{NO EVENT}}{3}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwe09boZZ81Y",
        "outputId": "b6f50c8a-bb97-4b4d-99be-6ce1f5e10946"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [01:07<10:10, 67.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: F1 Macro avg: 72.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [02:12<08:46, 65.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2: F1 Macro avg: 74.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [03:17<07:39, 65.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3: F1 Macro avg: 80.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [04:24<06:35, 65.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4: F1 Macro avg: 76.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [05:28<05:27, 65.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5: F1 Macro avg: 77.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 6/10 [06:35<04:23, 65.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 6: F1 Macro avg: 79.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 70%|███████   | 7/10 [07:39<03:15, 65.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 7: F1 Macro avg: 76.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 8/10 [08:43<02:09, 64.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 8: F1 Macro avg: 77.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 9/10 [09:47<01:04, 64.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 9: F1 Macro avg: 75.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:53<00:00, 65.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 10: F1 Macro avg: 75.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def choose_model(dict_data, folderPath):\n",
        "    from tqdm import tqdm\n",
        "    from sklearn.preprocessing import MaxAbsScaler\n",
        "    from sklearn.metrics import f1_score\n",
        "\n",
        "    threshold = 0.5\n",
        "    f1_macro = []\n",
        "    for fold in tqdm(range(1, 11)):\n",
        "        foldFolderPath = folderPath + str(fold) + \"/\"\n",
        "        \n",
        "        train_index = np.load(foldFolderPath + \"train_index.npy\")\n",
        "        validation_index = np.load(foldFolderPath + \"validation_index.npy\")\n",
        "\n",
        "        bestModel = ModelHandler.loadModel(foldFolderPath + \"model_class_opt.h5\", type_weights=None) # Load model\n",
        "\n",
        "        scaler = MaxAbsScaler()\n",
        "        scaler.fit(np.squeeze(dict_data[\"x_train\"][train_index], axis=2))\n",
        "        x_validation = np.expand_dims(scaler.transform(np.squeeze(dict_data[\"x_train\"][validation_index], axis=2)), axis=2)\n",
        "\n",
        "        final_prediction = []\n",
        "        final_groundTruth = []\n",
        "        for xi, yclass, ytype in zip(x_validation, dict_data[\"y_train\"][\"classification\"][validation_index], dict_data[\"y_train\"][\"type\"][validation_index]):\n",
        "            pred = bestModel.predict(np.expand_dims(xi, axis=0))\n",
        "            prediction = np.max(pred[2][0],axis=0)\n",
        "            groundTruth = np.max(yclass,axis=0)\n",
        "\n",
        "            final_prediction.append(prediction)\n",
        "            final_groundTruth.append(groundTruth) \n",
        "\n",
        "            del xi, yclass, ytype\n",
        "\n",
        "        event_type = np.min(np.argmax(dict_data[\"y_train\"][\"type\"][validation_index], axis=2), axis=1)\n",
        "\n",
        "        final_groundTruth = np.array(final_groundTruth)\n",
        "        final_prediction = np.array(final_prediction)\n",
        "    \n",
        "        f1_macro.append([f1_score(final_groundTruth[event_type == 0] > threshold, final_prediction[event_type == 0] > threshold, average='macro', zero_division=0), \n",
        "                         f1_score(final_groundTruth[event_type == 1] > threshold, final_prediction[event_type == 1] > threshold, average='macro', zero_division=0),\n",
        "                         f1_score(final_groundTruth[event_type == 2] > threshold, final_prediction[event_type == 2] > threshold, average='macro', zero_division=0)])\n",
        "        print(f\"Fold {fold}: F1 Macro avg: {np.average(f1_macro[-1]) * 100:.1f}\")\n",
        "\n",
        "    return np.argmax(np.average(f1_macro, axis=1)) + 1\n",
        "\n",
        "fold = choose_model(dict_data, folderPath)\n",
        "fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Scattering Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQDY22oqQvb3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "foldFolderPath = folderPath + str(fold) + \"/\"\n",
        "\n",
        "train_index = np.load(foldFolderPath + \"train_index.npy\")\n",
        "validation_index = np.load(foldFolderPath + \"validation_index.npy\")\n",
        "\n",
        "bestModel = ModelHandler.loadModel(foldFolderPath + \"model_class_opt.h5\", type_weights=None) # Load model\n",
        "scattering_extract = ModelHandler.loadModel(folderPath + \"scattering_model.h5\")\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "scaler.fit(np.squeeze(dict_data[\"x_train\"][train_index], axis=2))\n",
        "x_train = np.expand_dims(scaler.transform(np.squeeze(dict_data[\"x_train\"][train_index], axis=2)), axis=2)\n",
        "x_validation = np.expand_dims(scaler.transform(np.squeeze(dict_data[\"x_train\"][validation_index], axis=2)), axis=2)\n",
        "x_test = np.expand_dims(scaler.transform(np.squeeze(dict_data[\"x_test\"], axis=2)), axis=2)\n",
        "\n",
        "x_test = scattering_extract.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqYMsQ-oQvb1"
      },
      "source": [
        "## Evaluates the identification\n",
        "\n",
        "This step generates a dict with the ground truth and the prediction for each test example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_prediction = []\n",
        "final_groundTruth = []\n",
        "for xi, yclass, ytype in zip(x_test, dict_data[\"y_test\"][\"classification\"], dict_data[\"y_test\"][\"type\"]):\n",
        "    pred = bestModel.predict(np.expand_dims(xi, axis=0))\n",
        "    prediction = np.max(pred[2][0],axis=0)\n",
        "    groundTruth = np.max(yclass,axis=0)\n",
        "\n",
        "    final_prediction.append(prediction)\n",
        "    final_groundTruth.append(groundTruth) \n",
        "\n",
        "    del xi, yclass, ytype\n",
        "\n",
        "y = {}\n",
        "y[\"true\"] = final_groundTruth.copy()\n",
        "y[\"pred\"] = final_prediction.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usqx6Z-iQvb7"
      },
      "source": [
        "### F1 Score\n",
        "\n",
        "#### F1 Macro:\n",
        "$$\n",
        "\\begin{gather*}\n",
        "F1_{Macro} = \\frac{1}{Y} \\sum_{i=1}^{Y} \\frac{2 \\cdot tp_i}{2 \\cdot tp_i + fp_i + fn_i}\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "#### F1 Micro:\n",
        "$$\n",
        "\\begin{gather*}\n",
        "F1_{Micro} = \\frac{2 \\cdot \\sum_{i=1}^{Y} tp_i}{\\sum_{i=1}^{Y} 2 \\cdot tp_i + fp_i + fn_i}\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "- $tp_i$: True positives classifications for appliance $i$\n",
        "- $fp_i$: False positives classifications for appliance $i$\n",
        "- $fn_i$: False negatives classifications for appliance $i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP4MWwFLQvb9",
        "outputId": "aaf4cec1-e33c-478c-f1c5-dfa95bbc03e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 - F1 Macro: 83.2, F1 Micro: 82.9\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "threshold = 0.5\n",
        "f1_macro = f1_score(np.array(y[\"true\"]) > threshold, np.array(y[\"pred\"]) > threshold, average='macro')\n",
        "f1_micro = f1_score(np.array(y[\"true\"]) > threshold, np.array(y[\"pred\"]) > threshold, average='micro')\n",
        "\n",
        "print(f\"Fold {fold} - F1 Macro: {f1_macro * 100:.1f}, F1 Micro: {f1_micro * 100:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJVFvT3LQvcA"
      },
      "source": [
        "### Accuracy (ACC)\n",
        "\n",
        "$$\n",
        "\\begin{gather*}\n",
        "ACC_i = \\frac{CCE_i}{TNE_i} \\\\ \\\\\n",
        "ACC = \\frac{1}{Y} \\sum_{i = 1}^{Y} ACC_i\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "- $ACC_i$: Accuracy for appliance $i$\n",
        "- $CCE_i$: Load connected successfully identified\n",
        "- $TNE_i$: Total of connected events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQzNt8lTQvcD",
        "outputId": "a8b33f67-5ce7-4459-ac24-c28f050ea505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 - Acc on: 94.7, Acc off: 98.7, Acc no event: 99.4 Acc total: 98.5\n"
          ]
        }
      ],
      "source": [
        "threshold = 0.5\n",
        "\n",
        "correct_on = np.zeros((26,1))\n",
        "total_on = np.zeros((26,1))\n",
        "correct_off = np.zeros((26,1))\n",
        "total_off = np.zeros((26,1))\n",
        "correct_no_event = np.zeros((26,1))\n",
        "total_no_event = np.zeros((26,1))\n",
        "\n",
        "for ytype, ytrue, ypred in zip(dict_data[\"y_test\"][\"type\"], y[\"true\"], y[\"pred\"]):\n",
        "    event_type = np.min(np.argmax(ytype, axis=1))\n",
        "    if event_type == 0:\n",
        "        correct_on[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n",
        "        total_on[ytrue > threshold] += 1\n",
        "    elif event_type == 1:\n",
        "        correct_off[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n",
        "        total_off[ytrue > threshold] += 1\n",
        "    else:\n",
        "        correct_no_event[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n",
        "        total_no_event[ytrue > threshold] += 1\n",
        "\n",
        "acc_on = 100 * np.average(np.nan_to_num(correct_on/total_on))\n",
        "acc_off = 100 * np.average(np.nan_to_num(correct_off/total_off))\n",
        "acc_no_event = 100 * np.average(np.nan_to_num(correct_no_event/total_no_event))\n",
        "acc_total = 100 * np.average(np.nan_to_num((correct_on + correct_off + correct_no_event)/(total_on + total_off + total_no_event)))\n",
        "\n",
        "print(f\"Fold {fold} - Acc on: {acc_on:.1f}, Acc off: {acc_off:.1f}, Acc no event: {acc_no_event:.1f} Acc total: {acc_total:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFz4ZTJQvcH"
      },
      "source": [
        "## Detection Metrics\n",
        "\n",
        "### D\n",
        "$$\n",
        "\\begin{gather*}\n",
        "D = \\frac{ \\sum_{i=1}^{A} |d(i) - ev(i)|}{A}\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "- `A`: Total of events correctly detected ($\\pm$ 10 semi cycles tolerance)\n",
        "- `d(i)`: Detection for appliance $i$\n",
        "- `ev(i)`: Ground truth detection for appliance $i$\n",
        "\n",
        "## PC\n",
        "\n",
        "$$\n",
        "\\begin{gather*}\n",
        "PC = \\frac{A}{N}\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "- `A`: Total of events correctly detected ($\\pm$ 10 semi cycles tolerance)\n",
        "- `N`: Total of events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNHCZt9zQvcJ",
        "outputId": "2a3642b8-1555-4981-8d9e-7bb3d3483726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- FOLD 3 ---------------\n",
            "Total time: 69.59573629798615, Average Time: 0.033315335709902416\n",
            "LIT-SYN-1 PCmetric: (0.9428571428571428, 0.9523809523809523, 0.948051948051948)\n",
            "LIT-SYN-1 Dmetric: (1.1818181818181819, 1.075, 1.1232876712328768)\n",
            "LIT-SYN-2 PCmetric: (0.8968253968253969, 0.8832116788321168, 0.8897338403041825)\n",
            "LIT-SYN-2 Dmetric: (0.8849557522123894, 0.8429752066115702, 0.8632478632478633)\n",
            "LIT-SYN-3 PCmetric: (0.7987421383647799, 0.7554347826086957, 0.7755102040816326)\n",
            "LIT-SYN-3 Dmetric: (1.2440944881889764, 1.381294964028777, 1.3157894736842106)\n",
            "LIT-SYN-8 PCmetric: (0.5747126436781609, 0.5416666666666666, 0.559748427672956)\n",
            "LIT-SYN-8 Dmetric: (1.38, 1.5384615384615385, 1.449438202247191)\n",
            "LIT-SYN-All PCmetric: (0.7936117936117936, 0.7793103448275862, 0.7862232779097387)\n",
            "LIT-SYN-All Dmetric: (1.13312693498452, 1.1710914454277286, 1.1525679758308156)\n"
          ]
        }
      ],
      "source": [
        "from PostProcessing import PostProcessing\n",
        "from DataHandler import DataHandler\n",
        "\n",
        "postProcessing = PostProcessing(configs=configs)\n",
        "dataHandler = DataHandler(configs=configs)\n",
        "\n",
        "general_qtd_test = dict_data[\"y_test\"][\"group\"]\n",
        "\n",
        "print(f\"-------------- FOLD {fold} ---------------\")\n",
        "pcMetric, dMetric = postProcessing.checkModel(bestModel, x_test, dict_data[\"y_test\"], general_qtd=general_qtd_test, print_error=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "yolo_nilm_metrics_notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "metadata": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
